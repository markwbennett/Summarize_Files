#!/usr/bin/env python3
"""
PDF Summarization Tool
Concatenates PDFs, splits into chunks, and generates summaries using Claude API.
"""

import os
import sys
from pathlib import Path
from typing import List, Tuple
import PyPDF2
from anthropic import Anthropic
from dotenv import load_dotenv
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
import time
import signal
import pdfplumber
import pytesseract
from pdf2image import convert_from_path
from PIL import Image
import threading
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError
import subprocess
import multiprocessing

# Load environment variables
load_dotenv()

class PDFProcessor:
    def __init__(self, pdfs_folder: str = None):
        self.api_key = os.getenv('ANTHROPIC_API_KEY')
        if not self.api_key:
            raise ValueError("ANTHROPIC_API_KEY not found in environment variables")
        
        self.client = Anthropic(api_key=self.api_key)
        
        # Get PDF folder from user input if not provided
        if pdfs_folder is None:
            pdfs_folder = self.get_pdf_folder()
        
        self.pdfs_dir = Path(pdfs_folder)
        # Save outputs in the same folder as the PDFs
        self.output_dir = self.pdfs_dir
        self.summaries_dir = self.pdfs_dir / 'summaries'
        
        # Create summaries subdirectory if it doesn't exist
        self.summaries_dir.mkdir(exist_ok=True)
        
        # Check if Tesseract is available
        self.ocr_available = self._check_tesseract()
    
    def get_pdf_folder(self) -> str:
        """Prompt user for PDF folder location."""
        while True:
            folder_path = input("Enter the path to the folder containing PDF files: ").strip()
            
            if not folder_path:
                print("Please enter a valid folder path.")
                continue
            
            # Remove surrounding quotes if present
            if (folder_path.startswith('"') and folder_path.endswith('"')) or \
               (folder_path.startswith("'") and folder_path.endswith("'")):
                folder_path = folder_path[1:-1]
            
            # Expand user home directory if needed
            folder_path = os.path.expanduser(folder_path)
            
            # Convert to absolute path to handle relative paths
            folder_path = os.path.abspath(folder_path)
            
            if not os.path.exists(folder_path):
                print(f"Folder '{folder_path}' does not exist. Please try again.")
                continue
            
            if not os.path.isdir(folder_path):
                print(f"'{folder_path}' is not a directory. Please try again.")
                continue
            
            # Check if folder contains any PDF files
            pdf_files = list(Path(folder_path).glob('*.pdf'))
            if not pdf_files:
                print(f"No PDF files found in '{folder_path}'. Please try again.")
                continue
            
            print(f"‚úÖ Found {len(pdf_files)} PDF files in '{folder_path}'")
            for pdf in pdf_files[:5]:  # Show first 5 files
                print(f"   üìÑ {pdf.name}")
            if len(pdf_files) > 5:
                print(f"   ... and {len(pdf_files) - 5} more files")
            return folder_path
    
    def _check_tesseract(self) -> bool:
        """Check if Tesseract OCR is available."""
        try:
            pytesseract.get_tesseract_version()
            print("‚úÖ Tesseract OCR is available for image-based PDFs")
            return True
        except Exception:
            print("‚ö†Ô∏è  Tesseract OCR not found - image-only pages will be skipped")
            print("   Install with: brew install tesseract (Mac) or apt-get install tesseract-ocr (Linux)")
            return False
    
    def find_pdf_files(self) -> List[Path]:
        """Find all PDF files in the specified directory."""
        pdf_files = list(self.pdfs_dir.glob('*.pdf'))
        # Sort by filename for consistent ordering
        return sorted(pdf_files)
    
    def concatenate_pdfs(self, pdf_files: List[Path]) -> Path:
        """Concatenate multiple PDF files into one."""
        print(f"\nüîó Concatenating {len(pdf_files)} PDF files...")
        
        output_path = self.output_dir / 'concatenated.pdf'
        
        # Note: Previous outputs already cleaned up in process_pdfs()
        
        merger = PyPDF2.PdfMerger()
        
        for i, pdf_file in enumerate(pdf_files, 1):
            print(f"   üìé Adding ({i}/{len(pdf_files)}): {pdf_file.name}")
            try:
                with open(pdf_file, 'rb') as file:
                    merger.append(file)
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Warning: Could not add {pdf_file.name}: {e}")
                continue
        
        print(f"   üíæ Saving fresh concatenated PDF...")
        with open(output_path, 'wb') as output_file:
            merger.write(output_file)
        
        merger.close()
        print(f"‚úÖ Fresh concatenated PDF saved to: {output_path.name}")
        return output_path
    
    def count_pages(self, pdf_path: Path) -> int:
        """Count the number of pages in a PDF."""
        print(f"   üìä Counting pages in PDF...")
        try:
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                page_count = len(reader.pages)
                print(f"   ‚úÖ PDF contains {page_count} pages")
                return page_count
        except Exception as e:
            print(f"   ‚ùå Error reading PDF: {e}")
            raise
    
    def extract_text_from_pages(self, pdf_path: Path, start_page: int, end_page: int) -> str:
        """Extract text from a range of pages with fallback methods."""
        print(f"      üìñ Extracting text from pages {start_page+1} to {end_page}...")
        
        # Check if this PDF has known problematic characteristics
        if self._is_problematic_pdf(pdf_path):
            print(f"      ‚ö†Ô∏è  PDF appears problematic - skipping PyPDF2, using pdfplumber directly")
            return self._extract_with_pdfplumber(pdf_path, start_page, end_page)
        
        # Try PyPDF2 first
        try:
            return self._extract_with_pypdf2(pdf_path, start_page, end_page)
        except Exception as e:
            print(f"      ‚ö†Ô∏è  PyPDF2 failed: {e}")
            print(f"      üîÑ Trying alternative method with pdfplumber...")
            return self._extract_with_pdfplumber(pdf_path, start_page, end_page)
    
    def _is_problematic_pdf(self, pdf_path: Path) -> bool:
        """Check if PDF has characteristics that cause PyPDF2 to hang."""
        filename = pdf_path.name.lower()
        
        # AGHA Engineering business reports are definitively problematic
        if 'agha engineering' in filename and 'businessreport' in filename:
            print(f"      üö® Known problematic PDF: AGHA Engineering business report")
            return True
        
        # Very small business reports often have structural issues
        try:
            file_size_kb = pdf_path.stat().st_size / 1024
            if file_size_kb < 50 and 'businessreport' in filename:
                print(f"      ‚ö†Ô∏è  Small business report PDF ({file_size_kb:.1f}KB) - likely problematic")
                return True
            elif file_size_kb > 50000:  # Very large files (>50MB)
                print(f"      üìä Large PDF detected ({file_size_kb/1024:.1f}MB)")
                return True
        except:
            pass
        
        return False
    
    def _extract_with_pypdf2(self, pdf_path: Path, start_page: int, end_page: int) -> str:
        """Extract text using PyPDF2 with timeout protection."""
        with open(pdf_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            text = ""
            total_pages = min(end_page, len(reader.pages)) - start_page
            timeout_count = 0
            max_timeouts = 3  # If more than 3 pages timeout, give up on PyPDF2
            
            for i, page_num in enumerate(range(start_page, min(end_page, len(reader.pages)))):
                print(f"      üìÑ Processing page {page_num + 1} ({i + 1}/{total_pages})...")
                
                # Check if we've had too many timeouts
                if timeout_count >= max_timeouts:
                    print(f"         üö´ Too many PyPDF2 timeouts ({timeout_count}) - switching to pdfplumber")
                    raise ValueError(f"PyPDF2 had {timeout_count} timeouts, switching to pdfplumber")
                
                # Extract page text with timeout
                page_text = self._extract_single_page_with_timeout(reader, page_num, pdf_path)
                
                if page_text is None:
                    # PyPDF2 timed out, count it and skip this page
                    timeout_count += 1
                    print(f"         ‚è≠Ô∏è  Timeout #{timeout_count} - skipping page {page_num + 1} in PyPDF2")
                    continue
                elif not page_text or not page_text.strip():
                    print(f"         ‚ö†Ô∏è  Page {page_num + 1} appears to be empty or image-only")
                    # Try OCR if available
                    ocr_text = self._ocr_page(pdf_path, page_num)
                    if ocr_text:
                        page_text = ocr_text
                        print(f"         ‚úÖ OCR recovered text from page {page_num + 1}")
                
                if page_text:
                    text += page_text + "\n"
            
            if not text.strip():
                raise ValueError(f"No text extracted with PyPDF2")
            
            print(f"      ‚úÖ PyPDF2 extracted {len(text)} characters from {total_pages} pages")
            return text
    
    def _extract_single_page_with_timeout(self, reader, page_num: int, pdf_path: Path, timeout: int = 2) -> str:
        """Extract text from a single page with timeout protection."""
        print(f"         üîç Attempting PyPDF2 extraction on page {page_num + 1}...")
        
        # For very problematic pages, use process-based timeout
        if self._is_likely_problematic_page(pdf_path, page_num):
            print(f"         ‚ö†Ô∏è  Page {page_num + 1} appears to be problematic type - using process timeout")
            return self._extract_page_with_process_timeout(pdf_path, page_num, timeout)
        
        def extract_page():
            try:
                page = reader.pages[page_num]
                text = page.extract_text()
                print(f"         ‚úÖ PyPDF2 extraction completed for page {page_num + 1}")
                return text
            except Exception as e:
                print(f"         ‚ùå PyPDF2 error on page {page_num + 1}: {e}")
                return None
        
        try:
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(extract_page)
                result = future.result(timeout=timeout)
                return result if result is not None else ""
        except FuturesTimeoutError:
            print(f"         ‚è∞ PyPDF2 timeout on page {page_num + 1} (>{timeout}s)")
            print(f"         üîÑ Trying process-based extraction...")
            return self._extract_page_with_process_timeout(pdf_path, page_num, timeout)
        except Exception as e:
            print(f"         ‚ùå Unexpected error on page {page_num + 1}: {e}")
            return ""
    
    def _is_likely_problematic_page(self, pdf_path: Path, page_num: int) -> bool:
        """Check if a specific page is likely to be problematic."""
        # Known problematic file patterns
        filename = pdf_path.name.lower()
        
        # AGHA Engineering business reports are known to hang on first page
        if 'agha engineering' in filename and 'businessreport' in filename:
            if page_num == 0:  # First page (table of contents)
                print(f"         üö® Known problematic page: AGHA Engineering business report page 1")
                return True
        
        # General business report patterns
        if 'businessreport' in filename and page_num < 3:
            return True
            
        # Very small PDFs with few pages often have layout issues
        try:
            file_size_kb = pdf_path.stat().st_size / 1024
            if file_size_kb < 50 and page_num < 2:  # Small files, early pages
                print(f"         ‚ö†Ô∏è  Small PDF ({file_size_kb:.1f}KB) - early page may be problematic")
                return True
        except:
            pass
            
        return False
    
    def _extract_page_with_process_timeout(self, pdf_path: Path, page_num: int, timeout: int) -> str:
        """Extract page using OCR directly when PyPDF2 hangs."""
        print(f"         üö® Using OCR fallback for page {page_num + 1}")
        try:
            # Skip PyPDF2 entirely and go straight to OCR
            ocr_text = self._ocr_page(pdf_path, page_num)
            if ocr_text:
                print(f"         ‚úÖ OCR extracted text from problematic page {page_num + 1}")
                return ocr_text
            else:
                print(f"         ‚ö†Ô∏è  OCR found no text on page {page_num + 1}")
                return ""
        except Exception as e:
            print(f"         ‚ùå OCR also failed on page {page_num + 1}: {e}")
            return ""
    
    def _extract_pdfplumber_page_with_timeout(self, pdf, page_num: int, timeout: int = 10) -> str:
        """Extract text from a single page using pdfplumber with timeout protection."""
        def extract_page():
            try:
                page = pdf.pages[page_num]
                return page.extract_text()
            except Exception as e:
                print(f"         ‚ùå pdfplumber error on page {page_num + 1}: {e}")
                return None
        
        try:
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(extract_page)
                result = future.result(timeout=timeout)
                return result if result is not None else ""
        except FuturesTimeoutError:
            print(f"         ‚è∞ pdfplumber timeout on page {page_num + 1} (>{timeout}s) - skipping...")
            return ""
        except Exception as e:
            print(f"         ‚ùå Unexpected pdfplumber error on page {page_num + 1}: {e}")
            return ""
    
    def _extract_with_pdfplumber(self, pdf_path: Path, start_page: int, end_page: int) -> str:
        """Extract text using pdfplumber as fallback."""
        text = ""
        total_pages = end_page - start_page
        
        with pdfplumber.open(pdf_path) as pdf:
            for i, page_num in enumerate(range(start_page, min(end_page, len(pdf.pages)))):
                if i % 5 == 0 or i == total_pages - 1:
                    print(f"         Processing page {page_num + 1} ({i + 1}/{total_pages})...")
                
                # Extract page text with timeout
                page_text = self._extract_pdfplumber_page_with_timeout(pdf, page_num)
                
                if page_text and page_text.strip():
                    text += page_text + "\n"
                else:
                    print(f"         ‚ö†Ô∏è  Page {page_num + 1} appears to be empty or image-only")
                    # Try OCR if available
                    ocr_text = self._ocr_page(pdf_path, page_num)
                    if ocr_text:
                        text += ocr_text + "\n"
                        print(f"         ‚úÖ OCR recovered text from page {page_num + 1}")
        
        if not text.strip():
            raise ValueError(f"No text could be extracted from pages {start_page+1}-{end_page}. The PDF may be image-based or corrupted.")
        
        print(f"      ‚úÖ pdfplumber extracted {len(text)} characters from {total_pages} pages")
        return text
    
    def _ocr_page(self, pdf_path: Path, page_num: int) -> str:
        """OCR a single page if it appears to be image-only."""
        if not self.ocr_available:
            return ""
        
        try:
            print(f"         üîç Running OCR on page {page_num + 1}...")
            
            # Convert specific page to image
            images = convert_from_path(
                pdf_path, 
                first_page=page_num + 1, 
                last_page=page_num + 1,
                dpi=300,  # High DPI for better OCR accuracy
                fmt='JPEG'
            )
            
            if not images:
                return ""
            
            # OCR the image
            image = images[0]
            ocr_text = pytesseract.image_to_string(image, lang='eng')
            
            if ocr_text.strip():
                return ocr_text.strip()
            else:
                print(f"         ‚ö†Ô∏è  OCR found no text on page {page_num + 1}")
                return ""
                
        except Exception as e:
            print(f"         ‚ùå OCR failed for page {page_num + 1}: {e}")
            return ""
    
    def create_chunks(self, pdf_path: Path, max_pages: int = 100, overlap: int = 10) -> List[Tuple[int, int, str]]:
        """Split PDF into overlapping chunks."""
        print(f"\nüìä Analyzing document structure...")
        total_pages = self.count_pages(pdf_path)
        print(f"   üìñ Total pages: {total_pages}")
        
        if total_pages <= max_pages:
            # Single chunk
            print(f"   ‚úÖ Document fits in single chunk (‚â§{max_pages} pages)")
            try:
                text = self.extract_text_from_pages(pdf_path, 0, total_pages)
                return [(0, total_pages, text)]
            except Exception as e:
                print(f"   ‚ùå Error extracting text: {e}")
                raise
        
        print(f"   üìö Document requires chunking (>{max_pages} pages)")
        print(f"   ‚öôÔ∏è  Using {overlap}-page overlap between chunks")
        
        chunks = []
        start_page = 0
        chunk_num = 1
        
        while start_page < total_pages:
            end_page = min(start_page + max_pages, total_pages)
            print(f"   üîç Extracting chunk {chunk_num} (pages {start_page+1}-{end_page})...")
            try:
                text = self.extract_text_from_pages(pdf_path, start_page, end_page)
                chunks.append((start_page, end_page, text))
            except Exception as e:
                print(f"   ‚ùå Error extracting chunk {chunk_num}: {e}")
                # Continue with next chunk instead of failing completely
                print(f"   ‚ö†Ô∏è  Skipping chunk {chunk_num} and continuing...")
            
            # Move to next chunk with overlap
            start_page = end_page - overlap
            if start_page >= total_pages:
                break
            chunk_num += 1
        
        print(f"‚úÖ Created {len(chunks)} chunks for processing")
        return chunks
    
    def summarize_chunk(self, chunk_text: str, chunk_num: int, total_chunks: int) -> str:
        """Summarize a single chunk using Claude."""
        print(f"\nü§ñ Summarizing chunk {chunk_num + 1}/{total_chunks}...")
        print(f"   üìù Sending to Claude API...")
        
        prompt = f"""Please provide a comprehensive summary of this document chunk ({chunk_num + 1} of {total_chunks}).

Include:
1. Main topics and themes
2. Key events or developments
3. Important people mentioned
4. Significant dates or timeframes
5. Critical decisions or outcomes

Document text:
{chunk_text}"""
        
        try:
            start_time = time.time()
            response = self.client.messages.create(
                model="claude-3-sonnet-20240229",
                max_tokens=4000,
                messages=[{"role": "user", "content": prompt}]
            )
            elapsed_time = time.time() - start_time
            print(f"   ‚úÖ Summary completed in {elapsed_time:.1f}s")
            return response.content[0].text
        except Exception as e:
            print(f"   ‚ùå Error summarizing chunk {chunk_num + 1}: {e}")
            return f"Error summarizing chunk {chunk_num + 1}: {str(e)}"
    
    def generate_final_summary(self, chunk_summaries: List[str]) -> str:
        """Generate overall summary from chunk summaries."""
        print(f"\nüìã Generating final comprehensive summary...")
        print(f"   üîÑ Combining {len(chunk_summaries)} chunk summaries...")
        
        combined_summaries = "\n\n".join([f"Chunk {i+1} Summary:\n{summary}" 
                                        for i, summary in enumerate(chunk_summaries)])
        
        prompt = f"""Based on these chunk summaries of a larger document, please provide:

1. **OVERALL SUMMARY**: A comprehensive summary of the entire document set
2. **MAIN THEMES**: The key themes and topics that emerge across all chunks
3. **CRITICAL INSIGHTS**: The most important insights or conclusions

Chunk summaries:
{combined_summaries}"""
        
        try:
            print(f"   üìù Sending to Claude API...")
            start_time = time.time()
            response = self.client.messages.create(
                model="claude-3-sonnet-20240229",
                max_tokens=4000,
                messages=[{"role": "user", "content": prompt}]
            )
            elapsed_time = time.time() - start_time
            print(f"   ‚úÖ Final summary completed in {elapsed_time:.1f}s")
            return response.content[0].text
        except Exception as e:
            print(f"   ‚ùå Error generating final summary: {e}")
            return f"Error generating final summary: {str(e)}"
    
    def extract_timeline(self, chunk_summaries: List[str]) -> str:
        """Extract timeline from chunk summaries."""
        print(f"\n‚è∞ Extracting chronological timeline...")
        print(f"   üîç Analyzing temporal patterns...")
        
        combined_summaries = "\n\n".join([f"Chunk {i+1} Summary:\n{summary}" 
                                        for i, summary in enumerate(chunk_summaries)])
        
        prompt = f"""Based on these document summaries, please extract and create a chronological timeline of events.

Format as:
- Date/Period: Event description
- Date/Period: Event description

Include all significant dates, events, and developments mentioned across all chunks.
If exact dates aren't available, use approximate timeframes or relative chronology.

Chunk summaries:
{combined_summaries}"""
        
        try:
            print(f"   üìù Sending to Claude API...")
            start_time = time.time()
            response = self.client.messages.create(
                model="claude-3-sonnet-20240229",
                max_tokens=4000,
                messages=[{"role": "user", "content": prompt}]
            )
            elapsed_time = time.time() - start_time
            print(f"   ‚úÖ Timeline extraction completed in {elapsed_time:.1f}s")
            return response.content[0].text
        except Exception as e:
            print(f"   ‚ùå Error extracting timeline: {e}")
            return f"Error extracting timeline: {str(e)}"
    
    def extract_dramatis_personae(self, chunk_summaries: List[str]) -> str:
        """Extract dramatis personae from chunk summaries."""
        print(f"\nüë• Extracting dramatis personae...")
        print(f"   üîç Identifying key people and characters...")
        
        combined_summaries = "\n\n".join([f"Chunk {i+1} Summary:\n{summary}" 
                                        for i, summary in enumerate(chunk_summaries)])
        
        prompt = f"""Based on these document summaries, please create a dramatis personae (list of key people/characters).

Format as:
**Name** - Role/Title/Description of their significance and involvement

Include:
- All significant individuals mentioned
- Their roles, titles, or positions
- Brief description of their importance to the events/narrative
- Their relationships to other key figures (if relevant)

Chunk summaries:
{combined_summaries}"""
        
        try:
            print(f"   üìù Sending to Claude API...")
            start_time = time.time()
            response = self.client.messages.create(
                model="claude-3-sonnet-20240229",
                max_tokens=4000,
                messages=[{"role": "user", "content": prompt}]
            )
            elapsed_time = time.time() - start_time
            print(f"   ‚úÖ Character analysis completed in {elapsed_time:.1f}s")
            return response.content[0].text
        except Exception as e:
            print(f"   ‚ùå Error extracting dramatis personae: {e}")
            return f"Error extracting dramatis personae: {str(e)}"
    
    def create_pdf_summary(self, title: str, content: str, filename: str) -> Path:
        """Create a PDF file from text content."""
        print(f"   üìÑ Creating PDF: {filename}")
        
        output_path = self.output_dir / filename
        doc = SimpleDocTemplate(str(output_path), pagesize=letter,
                              rightMargin=72, leftMargin=72,
                              topMargin=72, bottomMargin=18)
        
        # Define styles
        styles = getSampleStyleSheet()
        title_style = ParagraphStyle(
            'CustomTitle',
            parent=styles['Heading1'],
            fontSize=16,
            spaceAfter=30,
            alignment=1  # Center alignment
        )
        
        # Build the document
        story = []
        
        # Add title
        story.append(Paragraph(title, title_style))
        story.append(Spacer(1, 12))
        
        # Split content into paragraphs and add to PDF
        paragraphs = content.split('\n\n')
        for para in paragraphs:
            if para.strip():
                # Handle bold text formatting
                para = para.replace('**', '<b>').replace('**', '</b>')
                # Create alternating bold tags
                bold_count = para.count('<b>')
                for i in range(0, bold_count, 2):
                    para = para.replace('<b>', '<b>', 1).replace('<b>', '</b>', 1)
                
                story.append(Paragraph(para, styles['Normal']))
                story.append(Spacer(1, 12))
        
        try:
            doc.build(story)
            print(f"   ‚úÖ PDF created: {filename}")
            return output_path
        except Exception as e:
            print(f"   ‚ùå Error creating PDF {filename}: {e}")
            # Fallback to text file
            text_path = output_path.with_suffix('.txt')
            with open(text_path, 'w', encoding='utf-8') as f:
                f.write(f"{title}\n{'='*len(title)}\n\n{content}")
            print(f"   üìù Saved as text file instead: {text_path.name}")
            return text_path
    
    def _cleanup_previous_outputs(self):
        """Clean up previous output files for a fresh run."""
        print("üßπ Cleaning up previous outputs...")
        
        # Files to clean up
        cleanup_files = [
            'concatenated.pdf',
            'final_summary.pdf',
            'timeline.pdf',
            'dramatis_personae.pdf'
        ]
        
        cleaned_count = 0
        for filename in cleanup_files:
            file_path = self.output_dir / filename
            if file_path.exists():
                file_path.unlink()
                cleaned_count += 1
        
        # Clean up chunk summaries
        chunk_pattern = self.output_dir.glob('chunk_*_summary.pdf')
        for chunk_file in chunk_pattern:
            chunk_file.unlink()
            cleaned_count += 1
        
        if cleaned_count > 0:
            print(f"   üóëÔ∏è  Removed {cleaned_count} previous output files")
        else:
            print("   ‚úÖ No previous outputs to clean")
    
    def process_pdfs(self):
        """Main processing function."""
        try:
            # Clean up any existing output files for fresh run
            self._cleanup_previous_outputs()
            
            # Find PDF files
            pdf_files = self.find_pdf_files()
            print(f"\nüéØ Processing {len(pdf_files)} PDF files")
            
            # Concatenate PDFs
            concatenated_pdf = self.concatenate_pdfs(pdf_files)
            
            # Create chunks
            chunks = self.create_chunks(concatenated_pdf)
            
            # Summarize each chunk
            print(f"\nüîÑ Processing {len(chunks)} chunks...")
            chunk_summaries = []
            for i, (start_page, end_page, text) in enumerate(chunks):
                summary = self.summarize_chunk(text, i, len(chunks))
                chunk_summaries.append(summary)
                
                # Save individual chunk summary as PDF
                chunk_title = f"Chunk {i+1} Summary (Pages {start_page+1}-{end_page})"
                chunk_filename = f'chunk_{i+1}_pages_{start_page+1}-{end_page}_summary.pdf'
                self.create_pdf_summary(chunk_title, summary, chunk_filename)
            
            print(f"\nüìä Generating comprehensive analysis...")
            
            # Generate final summary
            final_summary = self.generate_final_summary(chunk_summaries)
            print(f"   üíæ Saving final summary...")
            final_summary_file = self.create_pdf_summary(
                "Comprehensive Document Summary", 
                final_summary, 
                'final_summary.pdf'
            )
            
            # Extract timeline
            timeline = self.extract_timeline(chunk_summaries)
            print(f"   üíæ Saving timeline...")
            timeline_file = self.create_pdf_summary(
                "Chronological Timeline", 
                timeline, 
                'timeline.pdf'
            )
            
            # Extract dramatis personae
            dramatis_personae = self.extract_dramatis_personae(chunk_summaries)
            print(f"   üíæ Saving dramatis personae...")
            dramatis_file = self.create_pdf_summary(
                "Dramatis Personae", 
                dramatis_personae, 
                'dramatis_personae.pdf'
            )
            
            print(f"\nüéâ Processing complete!")
            print(f"üìÅ Results saved in: {self.output_dir}")
            print(f"üìÑ Generated files:")
            print(f"   ‚Ä¢ Concatenated PDF: concatenated.pdf")
            print(f"   ‚Ä¢ Final Summary: final_summary.pdf")
            print(f"   ‚Ä¢ Timeline: timeline.pdf")
            print(f"   ‚Ä¢ Character List: dramatis_personae.pdf")
            print(f"   ‚Ä¢ Individual Summaries: {len(chunks)} chunk summary PDFs")
            print(f"   ‚Ä¢ All files saved to: {self.output_dir.absolute()}")
            
        except Exception as e:
            print(f"‚ùå Error during processing: {e}")
            sys.exit(1)

def main():
    """Main entry point."""
    print("üöÄ PDF Summarization Tool")
    print("=" * 50)
    print("üìö This tool will:")
    print("   ‚Ä¢ Concatenate your PDF files")
    print("   ‚Ä¢ Split into chunks if needed (>100 pages)")
    print("   ‚Ä¢ Generate comprehensive summaries using Claude AI")
    print("   ‚Ä¢ Create timeline and character analysis")
    print("   ‚Ä¢ Save all results as PDF files")
    print("=" * 50)
    
    processor = PDFProcessor()
    processor.process_pdfs()

if __name__ == "__main__":
    main() 